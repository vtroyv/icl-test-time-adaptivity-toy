# In-Context Learning Adaptivity via Random Fourier Regression (Toy Study)

This repository contains a small empirical study investigating **test-time adaptivity in in-context learning (ICL)** using synthetic 1D regression tasks generated from random Fourier series.

The project is inspired by:

> Ma, Wang and Samworth (2025)  
> *Provable test-time adaptivity and distributional robustness of in-context learning*

This repo builds a minimal controlled setup to empirically illustrate key statistical phenomena discussed in the paper.

---

## Overview

We study whether a pretrained Transformer can:

1. Improve prediction accuracy as the number of in-context examples increases.
2. Adapt its performance to task smoothness (i.e., converge faster on easier tasks).

Each regression task is generated by sampling a random Fourier function:

\[
g(x) = \sum_{k=1}^{K} a_k \sin(2\pi k x) + b_k \cos(2\pi k x),
\]

where the coefficients decay according to

\[
a_k, b_k \sim \mathcal{N}(0, k^{-2\beta}),
\]

and the smoothness parameter **β** controls task difficulty:

- Smaller β → rougher functions (harder tasks)
- Larger β → smoother functions (easier tasks)

---

## Experimental Setup

- Input space: \( x \in [0,1] \)
- Output: \( y = g(x) + \varepsilon \), with Gaussian noise
- Tasks: Random Fourier series with varying smoothness β
- Model: Small encoder-only Transformer
- Training: Pretrained across many sampled tasks
- Evaluation:
  - Context sizes \( n \in \{4, 8, 16, 32, 64\} \)
  - Separate evaluation across different β values

---

## Key Empirical Results

### 1. Decreasing MSE with Context Size

For all smoothness levels β, we observe:

- Mean Squared Error (MSE) decreases as the number of in-context examples increases.
- The Transformer improves its predictions using additional context points.

This demonstrates basic in-context learning behaviour in a controlled regression setting.

---

### 2. Faster Convergence for Smoother Functions

Across different smoothness levels β, we observe:

- Smoother tasks (larger β) exhibit faster error reduction.
- Rougher tasks (smaller β) converge more slowly.

This provides empirical evidence that a single pretrained Transformer can adapt its performance to task difficulty without being explicitly told the smoothness level.

---


### Key Components

- **`task_sampler.py`**  
  Generates synthetic regression tasks using random Fourier series with controllable smoothness parameter β.

- **`model.py`**  
  Defines the encoder-only Transformer used for in-context regression.

- **`train.py`**  
  Handles pretraining over many sampled tasks drawn from the Fourier prior.

- **`evaluate.py`**  
  Evaluates Mean Squared Error (MSE) as a function of context size and smoothness level.

- **`src/notebooks/results.ipynb`**  
  Contains experimental results, plots, and a brief discussion of empirical findings:
  - MSE vs context size
  - Comparison across smoothness levels
  - Demonstration of test-time adaptivity

- **`requirements.txt`**  
  Lists Python dependencies required to reproduce the experiments.

## Takeaway

Even in a small synthetic setting, a pretrained Transformer can:

- Leverage additional context to reduce error.
- Adapt its convergence behaviour to underlying task smoothness.

This toy study provides a concrete illustration of test-time adaptivity in in-context learning.